# PoC RAG com Haystack, FAISS e LLMs

Este projeto √© uma Prova de Conceito (PoC) que demonstra a implementa√ß√£o de um pipeline de Retrieval-Augmented Generation (RAG) utilizando a biblioteca Haystack. Ele permite fazer perguntas sobre um documento de texto (`.txt` ou `.md`) e obter respostas geradas por um Modelo de Linguagem Grande (LLM) como GPT-4o Mini (OpenAI), Claude ou Gemini, com base no conte√∫do do documento fornecido.

## Pr√©-requisitos

| Ferramenta | Vers√£o recomendada | Observa√ß√£o |
|------------|-------------------|------------|
| **Python** | **3.11.x**        | Vers√µes 3.12+ ainda n√£o t√™m wheels est√°veis para *sentencepiece* (depend√™ncia de `sentence-transformers`) e exigem toolchain C/C++; use 3.11 para instalar sem problemas. |
| **pip**    | ‚â• 23              | `python -m pip install -U pip` |
| **Git**    | Qualquer          | Opcional, apenas para clonar o reposit√≥rio. |

> üí° Se precisar manter Python ‚â• 3.12, instale **Visual C++ Build Tools**, **CMake** e rode  
> `pip install --no-binary sentencepiece sentencepiece` antes de `requirements.txt`.  
> A PoC foi testada em Windows 10/11 e Linux (Ubuntu 22.04) com Python 3.11.

---

## Como Usar

1. **Crie e ative um ambiente virtual (recomendado):**

   *bash*
   ```bash
   python -m venv .venv
   source .venv/bin/activate
   ```
   *powershell*
   ```powershell
   py -3.11 -m venv .venv
   .venv\Scripts\Activate.ps1
   ```

2. **Instale as depend√™ncias:**
   ```bash
   python -m pip install -U pip            # garante o pip atualizado
   pip install -r requirements.txt         # instala Haystack, FAISS, etc.
   ```
   > Se surgir erro com *sentencepiece* em Python ‚â• 3.12, veja a nota em **Pr√©-requisitos**.

3. **Configure suas chaves de API** para o(s) provedor(es) desejado(s):
   ```powershell
   # exemplo em PowerShell (substitua pelas suas chaves)
   $Env:OPENAI_API_KEY = "sk-..."
   $Env:CLAUDE_API_KEY = "claude-..."
   $Env:GEMINI_API_KEY = "gemini-..."
   ```
   Defina apenas a(s) vari√°vel(is) que for utilizar.

---

## Como Executar

```bash
python rag.py --provider [openai|claude|gemini] --doc_path ks.txt
```

| Flag         | Descri√ß√£o                                                                                             |
|--------------|--------------------------------------------------------------------------------------------------------|
| `--provider` | Provedor de LLM a usar (`openai`, `claude` ou `gemini`).                                               |
| `--doc_path` | Caminho para o arquivo `.txt` ou `.md` que cont√©m o conte√∫do base.                                     |

**Exemplo**
```bash
python rag.py --provider openai --doc_path conhecimento_base.md
```

A primeira execu√ß√£o cria o √≠ndice FAISS; as seguintes reutilizam-no e iniciam mais r√°pido. Digite suas perguntas no terminal e `sair` para encerrar.

---

## Fluxo de Funcionamento

A aplica√ß√£o segue um fluxo RAG padr√£o orquestrado pelo Haystack:

1.  **Inicializa√ß√£o:**
    *   Ao executar o script pela primeira vez com um novo documento, ele l√™ o arquivo (`.txt` ou `.md`).
    *   Divide o texto em peda√ßos menores (chunks).
    *   Utiliza um modelo de embedding (`sentence-transformers/all-MiniLM-L6-v2`) para converter cada chunk em um vetor num√©rico (embedding).
    *   Armazena esses embeddings em um √≠ndice vetorial FAISS (`faiss_index/faiss_index.faiss` e `faiss_index.json`) e o texto original dos chunks em um banco de dados SQLite (`faiss_document_store.db`) gerenciados pelo `FAISSDocumentStore`.
    *   Se o √≠ndice j√° existir, ele √© carregado diretamente, pulando a etapa de processamento do documento.

2.  **Ciclo de Pergunta e Resposta:**
    *   **Usu√°rio:** Envia uma pergunta.
    *   **ConversationTracker:** Recupera o hist√≥rico recente da conversa (se houver).
    *   **Aplica√ß√£o:** Combina o hist√≥rico com a pergunta atual.
    *   **Retriever (`EmbeddingRetriever`):**
        *   Converte a pergunta combinada (com hist√≥rico) em um embedding usando o mesmo modelo (`all-MiniLM-L6-v2`).
        *   Consulta o `FAISSDocumentStore` para encontrar os embeddings de chunks mais similares ao embedding da pergunta (usando FAISS internamente).
        *   Retorna os chunks de texto correspondentes mais relevantes.
    *   **PromptNode:**
        *   Recebe a pergunta original, o hist√≥rico e os chunks relevantes recuperados.
        *   Formata um prompt final usando um template pr√©-definido, instruindo o LLM a responder com base no contexto fornecido (os chunks) e no hist√≥rico.
        *   Envia o prompt formatado para a API do LLM selecionado (OpenAI, Claude ou Gemini).
    *   **LLM API:** Processa o prompt e gera a resposta.
    *   **PromptNode:** Recebe a resposta do LLM.
    *   **Aplica√ß√£o:** Exibe a resposta ao usu√°rio.
    *   **ConversationTracker:** Armazena a pergunta e a resposta atual no hist√≥rico da sess√£o.

3.  **Encerramento:** O ciclo se repete at√© que o usu√°rio digite 'sair'.

### Diagrama de Sequ√™ncia: Inicializa√ß√£o

```mermaid
sequenceDiagram
    actor Usu√°rio
    participant Aplica√ß√£o as Aplica√ß√£o
    participant Retriever as Retriever (Haystack)
    participant DB_Vetorial as Document Store (FAISS)
    participant LLM_API as API LLM (OpenAI)

    Usu√°rio->>+Aplica√ß√£o: Inicia Aplica√ß√£o
    Aplica√ß√£o->>Aplica√ß√£o: Verifica exist√™ncia do √≠ndice
    alt √çndice N√£o Existe
        Aplica√ß√£o->>DB_Vetorial: Processa Documento e Cria √çndice
    else √çndice Existe
        Aplica√ß√£o->>DB_Vetorial: Carrega √çndice Existente
    end
    Aplica√ß√£o-->>Usu√°rio: Pronto para Perguntas
```

### Diagrama de Sequ√™ncia: Ciclo de Pergunta e Resposta

```mermaid
sequenceDiagram
    actor Usu√°rio
    participant Aplica√ß√£o as Aplica√ß√£o
    participant Retriever as Retriever (Haystack)
    participant DB_Vetorial as Document Store (FAISS)
    participant LLM_API as API LLM (OpenAI)

    loop Ciclo de Pergunta e Resposta
        Usu√°rio->>+Aplica√ß√£o: Envia Pergunta
        Aplica√ß√£o->>+Retriever: Buscar Documentos Relevantes
        Retriever->>+DB_Vetorial: Consulta por Similaridade (Embedding)
        DB_Vetorial-->>-Retriever: Retorna Documentos (Chunks)
        Retriever-->>-Aplica√ß√£o: Retorna Documentos Relevantes
        Aplica√ß√£o->>+LLM_API: Gerar Resposta (Pergunta + Documentos)
        LLM_API-->>-Aplica√ß√£o: Retorna Resposta Gerada
        Aplica√ß√£o-->>-Usu√°rio: Exibe Resposta
    end
```

## Gloss√°rio de Termos

*   **RAG (Retrieval-Augmented Generation):** Arquitetura de IA que melhora as respostas de LLMs ao primeiro recuperar informa√ß√µes relevantes de uma base de conhecimento externa (seus documentos) e depois usar essas informa√ß√µes como contexto para gerar a resposta.
*   **Haystack:** Framework open-source em Python para construir pipelines de busca sem√¢ntica e RAG. Fornece componentes como `DocumentStore`, `Retriever` e `PromptNode`.
*   **Pipeline:** No Haystack, uma sequ√™ncia de componentes (n√≥s) conectados que processam dados (ex: `Retriever` -> `PromptNode`).
*   **Document Store:** Componente do Haystack que armazena documentos e permite buscas eficientes.
*   **FAISS (Facebook AI Similarity Search):** Biblioteca otimizada para busca r√°pida de similaridade entre vetores de alta dimens√£o. Usada internamente pelo `FAISSDocumentStore`.
*   **FAISSDocumentStore:** Implementa√ß√£o espec√≠fica do `DocumentStore` no Haystack que usa:
    *   **FAISS:** Para criar e consultar um √≠ndice vetorial dos embeddings dos documentos (arquivos `faiss_index.faiss` e `faiss_index.json`).
    *   **SQLite:** Para armazenar o texto original e metadados dos documentos (arquivo `faiss_document_store.db`).
*   **Embedding:** Representa√ß√£o num√©rica (vetor) do significado sem√¢ntico de um texto. Textos similares possuem embeddings vetorialmente pr√≥ximos.
*   **Embedding Model (`sentence-transformers/all-MiniLM-L6-v2`):** Modelo de machine learning pr√©-treinado que converte texto em embeddings (vetores de 384 dimens√µes neste caso).
*   **Retriever (`EmbeddingRetriever`):** Componente l√≥gico do Haystack que atua como o "buscador inteligente". **N√£o √©** o banco vetorial em si, nem o FAISS, nem o LLM. Sua fun√ß√£o √©:
    1.  Receber a pergunta do usu√°rio (e hist√≥rico).
    2.  Usar o `Embedding Model` para converter a pergunta em um embedding.
    3.  Enviar esse embedding para o `FAISSDocumentStore`.
    4.  O `FAISSDocumentStore` utiliza o `FAISS` internamente para encontrar os embeddings de documentos mais similares.
    5.  Receber os documentos (chunks) correspondentes do `FAISSDocumentStore`.
    6.  Entregar esses documentos ao `PromptNode`.
*   **PromptNode:** Componente do Haystack que interage com um LLM. Ele pega a pergunta, os documentos recuperados pelo `Retriever` e o hist√≥rico, formata tudo usando um `Prompt Template`, e envia para a API do LLM (OpenAI, Claude, Gemini) para gerar a resposta final.
*   **Prompt Template:** Modelo de texto que define como as informa√ß√µes (contexto, hist√≥rico, pergunta) devem ser apresentadas ao LLM para gui√°-lo na gera√ß√£o da resposta.
*   **LLM (Large Language Model):** Modelo de IA (ex: GPT-4o Mini, Claude, Gemini) treinado para entender e gerar linguagem natural. No RAG, ele gera a resposta final com base no prompt enriquecido pelo `PromptNode`.
*   **Chunks:** Peda√ßos menores em que o documento original √© dividido para melhor processamento, embedding e recupera√ß√£o.
*   **ConversationTracker:** Classe auxiliar neste script para manter um hist√≥rico das √∫ltimas intera√ß√µes (perguntas e respostas) e fornecer contexto conversacional ao LLM.