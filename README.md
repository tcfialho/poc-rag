# PoC RAG com Haystack, Chroma e OpenRouter

Este projeto √© uma Prova de Conceito (PoC) que demonstra a implementa√ß√£o de um pipeline de Retrieval-Augmented Generation (RAG) utilizando a biblioteca Haystack. Ele permite fazer perguntas sobre um documento de texto (`.txt` ou `.md`) e obter respostas geradas por um Modelo de Linguagem Grande (LLM) via OpenRouter, com base no conte√∫do do documento fornecido.

## Pr√©-requisitos

| Ferramenta | Vers√£o recomendada | Observa√ß√£o |
|------------|-------------------|------------|
| **Python** | **3.11.x**        | Vers√µes 3.12+ ainda n√£o t√™m wheels est√°veis para *sentencepiece* (depend√™ncia de `sentence-transformers`) e exigem toolchain C/C++; use 3.11 para instalar sem problemas. |
| **pip**    | ‚â• 23              | `python -m pip install -U pip` |
| **Git**    | Qualquer          | Opcional, apenas para clonar o reposit√≥rio. |

> üí° Se precisar manter Python ‚â• 3.12, instale **Visual C++ Build Tools**, **CMake** e rode  
> `pip install --no-binary sentencepiece sentencepiece` antes de `requirements.txt`.  
> A PoC foi testada em Windows 10/11 e Linux (Ubuntu 22.04) com Python 3.11.

---

## Como Usar

1. **Crie e ative um ambiente virtual (recomendado):**

   *bash*
   ```bash
   python -m venv .venv
   source .venv/bin/activate
   ```
   *powershell*
   ```powershell
   py -3.11 -m venv .venv
   .venv\Scripts\Activate.ps1
   ```

2. **Instale as depend√™ncias:**
   ```bash
   python -m pip install -U pip            # garante o pip atualizado
   pip install -r requirements.txt         # instala Haystack, FAISS, etc.
   ```
   > Se surgir erro com *sentencepiece* em Python ‚â• 3.12, veja a nota em **Pr√©-requisitos**.

3. **Configure sua chave de API OpenRouter:**

   Voc√™ pode definir a chave de API de tr√™s maneiras (em ordem de preced√™ncia):
   1.  **Argumento de linha de comando:** Passe `--openrouter-api-key "sk-..."` ao executar o script.
   2.  **Vari√°vel de ambiente:** Defina a vari√°vel de ambiente `OPENROUTER_API_KEY`.
   3.  **Entrada interativa:** Se a chave n√£o for fornecida pelas op√ß√µes acima, o script solicitar√° que voc√™ a insira no terminal.

   *Exemplo em PowerShell para vari√°vel de ambiente:*
   ```powershell
   $Env:OPENROUTER_API_KEY = "sk-or-v1-..."
   ```

---

## Como Executar

```bash
python rag.py --doc_path ks.txt --openrouter-api-key "sk-or-v1-..."
```

| Flag                  | Descri√ß√£o                                                                                             |
|-----------------------|--------------------------------------------------------------------------------------------------------|
| `--doc_path`          | Caminho para o arquivo `.txt` ou `.md` que cont√©m o conte√∫do base.                                     |
| `--openrouter-api-key`| Sua chave de API OpenRouter (opcional). Se n√£o fornecida, tenta vari√°vel de ambiente ou solicita interativamente. |

**Exemplo**
```bash
python rag.py --doc_path conhecimento_base.md --openrouter-api-key "sk-or-v1-..."
```

A primeira execu√ß√£o cria o √≠ndice Chroma; as seguintes reutilizam-no e iniciam mais r√°pido. Digite suas perguntas no terminal e `sair` para encerrar.

---

## Fluxo de Funcionamento

A aplica√ß√£o segue um fluxo RAG padr√£o orquestrado pelo Haystack:

1.  **Inicializa√ß√£o:**
    *   Ao executar o script pela primeira vez com um novo documento, ele l√™ o arquivo (`.txt` ou `.md`).
    *   Divide o texto em peda√ßos menores (chunks).
    *   Utiliza um modelo de embedding (`sentence-transformers/all-MiniLM-L6-v2`) para converter cada chunk em um vetor num√©rico (embedding).
    *   Armazena esses embeddings em um `ChromaDocumentStore` persistente (`./chroma_db`).
    *   Se o √≠ndice j√° existir, ele √© carregado diretamente, pulando a etapa de processamento do documento.

2.  **Ciclo de Pergunta e Resposta:**
    *   **Usu√°rio:** Envia uma pergunta.
    *   **ConversationTracker:** Recupera o hist√≥rico recente da conversa (se houver).
    *   **Aplica√ß√£o:** Combina o hist√≥rico com a pergunta atual.
    *   **Retriever (`ChromaEmbeddingRetriever`):**
        *   Converte a pergunta combinada (com hist√≥rico) em um embedding usando o mesmo modelo (`all-MiniLM-L6-v2`).
        *   Consulta o `ChromaDocumentStore` para encontrar os embeddings de chunks mais similares ao embedding da pergunta.
        *   Retorna os chunks de texto correspondentes mais relevantes.
    *   **ChatPromptBuilder:**
        *   Recebe a pergunta original, o hist√≥rico e os chunks relevantes recuperados.
        *   Formata um prompt final usando um template pr√©-definido, instruindo o LLM a responder com base no contexto fornecido (os chunks) e no hist√≥rico.
        *   Envia o prompt formatado para a API do LLM (OpenRouter).
    *   **LLM API:** Processa o prompt e gera a resposta.
    *   **Aplica√ß√£o:** Exibe a resposta ao usu√°rio.
    *   **ConversationTracker:** Armazena a pergunta e a resposta atual no hist√≥rico da sess√£o.

3.  **Encerramento:** O ciclo se repete at√© que o usu√°rio digite 'sair'.

### Diagrama de Sequ√™ncia: Ingest√£o / Inicializa√ß√£o

```mermaid
sequenceDiagram
    actor Usu√°rio
    participant Aplica√ß√£o as Aplica√ß√£o
    participant Retriever as Retriever (Haystack)
    participant DB_Vetorial as Document Store (Chroma)
    participant LLM_API as API LLM (OpenRouter)

    Usu√°rio->>+Aplica√ß√£o: Inicia Aplica√ß√£o
    Aplica√ß√£o->>Aplica√ß√£o: Verifica exist√™ncia do √≠ndice
    alt √çndice N√£o Existe
        Aplica√ß√£o->>DB_Vetorial: Processa Documento e Cria √çndice
    else √çndice Existe
        Aplica√ß√£o->>DB_Vetorial: Carrega √çndice Existente
    end
    Aplica√ß√£o-->>Usu√°rio: Pronto para Perguntas
```

### Diagrama de Sequ√™ncia: Ciclo de Pergunta e Resposta

```mermaid
sequenceDiagram
    actor Usu√°rio
    participant Aplica√ß√£o as Aplica√ß√£o
    participant Retriever as Retriever (Haystack)
    participant DB_Vetorial as Document Store (Chroma)
    participant LLM_API as API LLM (OpenRouter)

    loop Ciclo de Pergunta e Resposta
        Usu√°rio->>+Aplica√ß√£o: Envia Pergunta
        Aplica√ß√£o->>+Retriever: Buscar Documentos Relevantes
        Retriever->>+DB_Vetorial: Consulta por Similaridade (Embedding)
        DB_Vetorial-->>-Retriever: Retorna Documentos (Chunks)
        Retriever-->>-Aplica√ß√£o: Retorna Documentos Relevantes
        Aplica√ß√£o->>+LLM_API: Gerar Resposta (Pergunta + Documentos)
        LLM_API-->>-Aplica√ß√£o: Retorna Resposta Gerada
        Aplica√ß√£o-->>-Usu√°rio: Exibe Resposta
    end
```

## Gloss√°rio de Termos

*   **RAG (Retrieval-Augmented Generation):** Arquitetura de IA que melhora as respostas de LLMs ao primeiro recuperar informa√ß√µes relevantes de uma base de conhecimento externa (seus documentos) e depois usar essas informa√ß√µes como contexto para gerar a resposta.
*   **Haystack:** Framework open-source em Python para construir pipelines de busca sem√¢ntica e RAG. Fornece componentes como `DocumentStore`, `Retriever` e `ChatPromptBuilder`.
*   **Pipeline:** No Haystack, uma sequ√™ncia de componentes (n√≥s) conectados que processam dados (ex: `Retriever` -> `ChatPromptBuilder`).
*   **Document Store:** Componente do Haystack que armazena documentos e permite buscas eficientes.
*   **Chroma:** Banco de dados vetorial de c√≥digo aberto que facilita o armazenamento e a busca de embeddings. Usado internamente pelo `ChromaDocumentStore`.
*   **ChromaDocumentStore:** Implementa√ß√£o espec√≠fica do `DocumentStore` no Haystack que usa Chroma para armazenar e consultar embeddings e documentos.
*   **Embedding:** Representa√ß√£o num√©rica (vetor) do significado sem√¢ntico de um texto. Textos similares possuem embeddings vetorialmente pr√≥ximos.
*   **Embedding Model (`sentence-transformers/all-MiniLM-L6-v2`):** Modelo de machine learning pr√©-treinado que converte texto em embeddings (vetores de 384 dimens√µes neste caso).
*   **Retriever (`ChromaEmbeddingRetriever`):** Componente l√≥gico do Haystack que atua como o "buscador inteligente". **N√£o √©** o banco vetorial em si, nem o Chroma, nem o LLM. Sua fun√ß√£o √©:
    1.  Receber a pergunta do usu√°rio (e hist√≥rico).
    2.  Usar o `Embedding Model` para converter a pergunta em um embedding.
    3.  Enviar esse embedding para o `ChromaDocumentStore`.
    4.  O `ChromaDocumentStore` utiliza o Chroma internamente para encontrar os embeddings de documentos mais similares.
    5.  Receber os documentos (chunks) correspondentes do `ChromaDocumentStore`.
    6.  Entregar esses documentos ao `ChatPromptBuilder`.
*   **ChatPromptBuilder:** Componente do Haystack que interage com um LLM. Ele pega a pergunta, os documentos recuperados pelo `Retriever` e o hist√≥rico, formata tudo usando um `Prompt Template`, e envia para a API do LLM (OpenRouter) para gerar a resposta final.
*   **Prompt Template:** Modelo de texto que define como as informa√ß√µes (contexto, hist√≥rico, pergunta) devem ser apresentadas ao LLM para gui√°-lo na gera√ß√£o da resposta.
*   **LLM (Large Language Model):** Modelo de IA (ex: modelos OpenRouter) treinado para entender e gerar linguagem natural. No RAG, ele gera a resposta final com base no prompt enriquecido pelo `ChatPromptBuilder`.
*   **Chunks:** Peda√ßos menores em que o documento original √© dividido para melhor processamento, embedding e recupera√ß√£o.
*   **ConversationTracker:** Classe auxiliar neste script para manter um hist√≥rico das √∫ltimas intera√ß√µes (perguntas e respostas) e fornecer contexto conversacional ao LLM.